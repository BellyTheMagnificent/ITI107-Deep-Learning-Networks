{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-pdc2-students/blob/master/iti107/session-4/object_detection_yolov2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with YOLOv2\n",
    "\n",
    "Welcome to your object detection programming exercise. This exercise allows you to implement some of the main ideas we covered in the lecture for YOLOv2 algorithm, such as bounding boxes, IOU and non-max-suppression. \n",
    "\n",
    "**You will learn how to:**\n",
    "- apply object detection on image and video\n",
    "- implement IoU, Non-Max-Suppression \n",
    "\n",
    "**Note**: Please run this using Tensorflow 1.14 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:52.056478",
     "start_time": "2018-04-04T00:18:50.879887"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imgaug as ia\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, cv2\n",
    "from darknet import build_darknet\n",
    "from utils import sigmoid, softmax, draw_boxes\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Build he YOLO network (Darknet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:53.978220",
     "start_time": "2018-04-04T00:18:53.967537"
    }
   },
   "outputs": [],
   "source": [
    "model = build_darknet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "source": [
    "**Exercise**: \n",
    "\n",
    "Examine the model using model.summary()\n",
    "\n",
    "1. What is the input shape (of the image) for the Yolo's Darknet? \n",
    "\n",
    "2. What is the output shape of the the Yolo's Darknet\n",
    "\n",
    "3. What does the last axis of the Yolo output contain? \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. (416, 416, 3)\n",
    "2. (13, 13, 5, 85)\n",
    "3. The last axis has 85 elements, which are: confidence_score, x,y,w,h of the bounding boxes, and 80 class probabalities\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained weights for YOLOv2 converted to .h5 format (pre-trained on COCO dataset) can be downloaded from: \n",
    "\n",
    "https://sdaaidata.s3-ap-southeast-1.amazonaws.com/pretrained-weights/full_yolov2.h5\n",
    "\n",
    "The original YOLO pretrained weights and config file can be downloaded from YOLO's author website: \n",
    "\n",
    "https://pjreddie.com/media/files/yolov2.weights\n",
    "\n",
    "However this weight file need to be converted to .h5 format before it can loaded using keras's ```model.load_weights()``` method. \n",
    "\n",
    "Refer to this link on how to convert the weights:\n",
    "\n",
    "https://github.com/allanzelener/YAD2K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this only if you have wget installed on your Linux system\n",
    "! wget https://sdaaidata.s3-ap-southeast-1.amazonaws.com/pretrained-weights/iti107/session-4/full_yolov2.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"full_yolov2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process the detection output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Anchor boxes** in YOLO allows multiple objects to be detected within a grid cell.  YOLOv2 uses 5 anchor boxes which are shown below. Each pair of numbers represent the width and height of a single anchor box. The dimension is relative to a grid cell. \n",
    "- **Object Threshold** controls which boxes to keep based on confidence score (in the corresponding grid) and class probabalities\n",
    "- **NMS threshold** is the iou threshold used to decided whether to remove a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:52.075535",
     "start_time": "2018-04-04T00:18:52.057712"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 80\n",
    "OBJ_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.45\n",
    "ANCHOR_BOXES = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the class labels from the file labels.txt (this will contain all the names for the 80 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the class labels YOLOv2 is trained on\n",
    "labels = [label.rstrip('\\n') for label in open('labels.txt')]\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a convenient class to hold the information about each bounding box predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c=None, class_prob_score=None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "            \n",
    "        # This is the confidence score\n",
    "        self.confidence = c\n",
    "        # This is the class probabalities score (confidence score * class probabilities)\n",
    "        self.class_prob_scores = class_prob_score\n",
    "\n",
    "        self.label = -1\n",
    "        self.class_prob_score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            # return the class label corresponding to the highest class probability score\n",
    "            self.label = np.argmax(self.class_prob_scores)\n",
    "\n",
    "        return self.label\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.class_prob_score == -1:\n",
    "            self.class_prob_score = self.class_prob_scores[self.get_label()]\n",
    "\n",
    "        return self.class_prob_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the detection layer (netout) is of the shape 13 x 13 x 5 x 85\n",
    "\n",
    "It'll be convenient to rearrange the (13,13,5,85) tensor into the following variables:  \n",
    "- `box_confidence`: tensor of shape $(13 \\times 13, 5, 1)$ containing $p_c$ (confidence probability that there's some object) for each of the 5 boxes predicted in each of the 19x19 cells.\n",
    "- `boxes`: tensor of shape $(13 \\times 13, 5, 4)$ containing $(b_x, b_y, b_h, b_w)$ for each of the 5 boxes per cell.\n",
    "- `box_class_probs`: tensor of shape $(13 \\times 13, 5, 80)$ containing the detection probabilities $(c_1, c_2, ... c_{80})$ for each of the 80 classes for each of the 5 boxes per cell.\n",
    "\n",
    "\n",
    "**Exercise** \n",
    "\n",
    "***This is quite a challenging exercise. Don't worry if you do not know how to do it :)***\n",
    "- Retrieve the box confidence from the netout and apply sigmoid function `sigmoid(x)` so that the confidence score is between 0 and 1\n",
    "- Retrieve the box class probabilities (80 of them) and apply function `softmax(x)`  so that the sum of the classes sums to 1.0\n",
    "- Compute the scores of class probabilities by multiplying (element-wise) the confidence score with class probabalities (see the diagram below for clearer picture of how this is done)\n",
    "- For those with score < threshold, set it to 0, so that the associated box will be removed later\n",
    "\n",
    "<img src=\"nb_images/probability_extraction.png\" style=\"width:500px;height:400;\"/>\n",
    "\n",
    "***Hint***:\n",
    "\n",
    "`netout` is of shape (13, 13, 5, 85). To quickly access the elements of the last axis, you can use the ellipsis in numpy array to expand to the number of ':' objects needed to make a selection tuple of the same length as x.ndim. \n",
    "\n",
    "e.g. Given an numpy array x of shape (2,3,2): \n",
    "```\n",
    "[[[ 0  1]\n",
    "  [ 2  3]\n",
    "  [ 4  5]]\n",
    "\n",
    " [[ 6  7]\n",
    "  [ 8  9]\n",
    "  [10 11]]]\n",
    "```\n",
    "``x[..., 0]`` will give  ``[[0,2,4], [6,8,10]]`` and ``x[..., 1]`` will give ``[[1,3,5], [7,9,11]]``\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "def filter_boxes(netout, obj_threshold=0.3):\n",
    "    \n",
    "    boxes = []\n",
    "   \n",
    "    box_confidences = netout[..., 4] \n",
    "    box_confidences = sigmoid(box_confidences)\n",
    "\n",
    "    # make the box_confidences the same number of axis as box_class_probs so you can multiply them together\n",
    "    box_confidences = box_confidences[..., np.newaxis]\n",
    "    \n",
    "    box_class_probs = netout[..., 5:]   # 5th element onwards are individual class probs\n",
    "    box_class_probs = softmax(box_class_probs)\n",
    "    \n",
    "    # Compute box clas prob scores by doing the elementwise product of box_confidences and box_class_probs\n",
    "    # You need both box_confidnences and box_class_probs to have the same number of axis\n",
    "    box_scores = box_confidences * box_class_probs\n",
    "    \n",
    "    # for class probablies less than threshold, set it to 0 other set it to 1\n",
    "    box_scores *= box_scores > obj_threshold\n",
    "    \n",
    "    return box_confidences, box_class_probs, box_scores\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(netout, obj_threshold=0.3):\n",
    "    \n",
    "    boxes = []\n",
    "    \n",
    "    # look at the last axis which is the one with 85 elements\n",
    "    # out of these 85, 5 is (x,y, w, h, confidence)\n",
    "    # element at index 4 (5th element) is the confidence score\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    box_confidences = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # make the box_confidences the same number of axis as box_class_probs so you can multiply them together\n",
    "    box_confidences = box_confidences[..., np.newaxis]\n",
    "    \n",
    "    # The last axis's 5th element onwards are individual class probs\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    box_class_probs = None   \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Compute box clas prob scores by doing the elementwise product of box_confidences and box_class_probs\n",
    "    # You need both box_confidnences and box_class_probs to have the same number of axis\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    box_scores = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # for class probablies less than threshold, set it to 0 other set it to 1\n",
    "    box_scores *= box_scores > obj_threshold\n",
    "    \n",
    "    return box_confidences, box_class_probs, box_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes loops through each of the 13 x 13 grid cells, and for each of the 5 predicted boxes of each grid cell, compute the bounding box's x_min, y_min (top left corner) and x_max, y_max (bottom left corner) using the following formula (reproduced from YOLOv2 paper), where $p_w$ and $p_h$ are the width and height of the corresponding anchor box, and $t_x$, $t_y$, $t_w$ and $t_h$ are the 4 coordinates of each bounding box, and $c_x$, $c_y$ is the offset from top-left corner of the image (corresponds to grid location):\n",
    "\n",
    "<img src=\"nb_images/bounding_box_location.png\" style=\"width:150px;height:100\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
    "    boxes = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # call the filter_box() to get box_confidences, box_class_probs and box_scores\n",
    "    box_confidences, box_class_probs, box_scores = filter_boxes(netout, obj_threshold)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
    "    \n",
    "    # calculate the locations of each of the 5 bounding boxes for each of the 13 x 13 locations\n",
    "    count = 0\n",
    "    for row in range(grid_h):\n",
    "        for col in range(grid_w):\n",
    "            for b in range(nb_box):\n",
    "                # from 4th element onwards are confidence and class classes\n",
    "                box_score = box_scores[row,col,b]\n",
    "                \n",
    "                # if scores for all classes are 0, then skip the box\n",
    "                if np.sum(box_score) > 0:\n",
    "                    # first 4 elements are x, y, w, and h\n",
    "                    x, y, w, h = netout[row,col,b,:4]\n",
    "            \n",
    "                    # x that is output is relative to each cell, so need to compute the \n",
    "                    # x, and y is the coordinate of the center of the bounding \n",
    "                    x = (col + sigmoid(x)) / grid_w # center position, unit: image width\n",
    "                    y = (row + sigmoid(y)) / grid_h # center position, unit: image height\n",
    "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
    "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
    "                    \n",
    "                    confidence = box_confidences[row,col,b]\n",
    "                    \n",
    "                    # convert the coordinate to top/left corner and bottom/right corner \n",
    "                    x_min = x - w/2\n",
    "                    x_max = x + w/2 \n",
    "                    y_min = y - h/2\n",
    "                    y_max = y + h/2 \n",
    "                    \n",
    "                    box = BoundBox(x_min, y_min, x_max, y_max, confidence, box_score)\n",
    "\n",
    "                    boxes.append(box)\n",
    "                    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "\n",
    "Non-max suppression uses a very important function called **\"Intersection over Union\"**, or IoU.\n",
    "<img src=\"nb_images/iou.png\" style=\"width:500px;height:400;\"/>\n",
    "<caption><center> Definition of \"Intersection over Union\"<br> </center></caption>\n",
    "\n",
    "**Exercise**: Implement bbox_iou(). \n",
    "\n",
    "Some hints:\n",
    "- In this exercise only, we define a box using its two corners (upper left and lower right): `(xmin, ymin, xmax, ymax)` rather than the midpoint and height/width.\n",
    "- To calculate the area of a rectangle you need to multiply its height `(ymax - ymin)` by its width `(xmax - xmin)`.\n",
    "- You'll also need to find the coordinates `(x1_i, y1_i, x2_i, y2_i)` of the intersection of two boxes. \n",
    "\n",
    "Remember that:\n",
    "    - x1_i = maximum of the x1 coordinates of the two boxes\n",
    "    - y1_i = maximum of the y1 coordinates of the two boxes\n",
    "    - x2_i = minimum of the x2 coordinates of the two boxes\n",
    "    - y2_i = minimum of the y2 coordinates of the two boxes\n",
    "    \n",
    "- In order to compute the intersection area, you need to make sure the height and width of the intersection are positive, otherwise the intersection area should be zero. Use `max(height, 0)` and `max(width, 0)`.\n",
    "\n",
    "In this code, we use the convention that (0,0) is the top-left corner of an image, (1,0) is the upper-right corner, and (1,1) the lower-right corner. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
    "    \n",
    "    Arguments:\n",
    "    box1 -- first box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    box2 -- second box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the intersection\n",
    "    x1_i = max(box1.xmin, box2.xmin)  \n",
    "    y1_i = max(box1.ymin, box2.ymin)\n",
    "    x2_i = min(box1.xmax, box2.xmax)\n",
    "    y2_i = min(box1.ymax, box2.ymax)\n",
    "    intersection_w = max(x2_i - x1_i, 0)\n",
    "    intersection_h = max(y2_i - y1_i, 0)\n",
    "    intersection_area = intersection_w * intersection_h\n",
    "    \n",
    "    # calculate the union \n",
    "    box1_area = (box1.xmax - box1.xmin) * (box1.ymax - box1.ymin)\n",
    "    box2_area = (box2.xmax - box2.xmin) * (box2.ymax - box2.ymin)\n",
    "    \n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    iou = float(intersection_area)/union_area\n",
    "    \n",
    "    return iou\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
    "    \n",
    "    Arguments:\n",
    "    box1 -- first box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    box2 -- second box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    # calculate the intersection\n",
    "    \n",
    "    \n",
    "    # calculate the union \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE HERE ###\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Max Suppression\n",
    "\n",
    "Here is the code that implement non-max suppression. The key steps are: \n",
    "1. Select the box that has the highest score.\n",
    "2. Compute the overlap of this box with all other boxes, and remove boxes that overlap significantly (iou >= `iou_threshold`).\n",
    "3. Go back to step 1 and iterate until there are no more boxes with a lower score than the currently selected box.\n",
    "\n",
    "This will remove all boxes that have a large overlap with the selected boxes. Only the \"best\" boxes remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, nb_class, nms_threshold, obj_threshold):\n",
    "\n",
    "    # np.argsort sorts in ascending order, we reverse so we will look at box with highest probablies\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = list(reversed(np.argsort([box.class_prob_scores[c] for box in boxes])))\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            \n",
    "            if boxes[index_i].class_prob_scores[c] == 0: \n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(sorted_indices)):\n",
    "                    index_j = sorted_indices[j]\n",
    "                    \n",
    "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
    "                        boxes[index_j].class_prob_scores[c] = 0\n",
    "                        \n",
    "    # remove the boxes which are less likely than obj_threshold\n",
    "    boxes = [box for box in boxes if box.get_score() > 0]\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform detection on image\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Before you give the image to the model, it needs to be preprocessed as follows:\n",
    "\n",
    "1. resize the image to the correct input size expected by the model (recall the input size from Part 2)\n",
    "2. Normalize values of each pixel to between (0,1)\n",
    "3. Reverse the channel order (if necessary) \n",
    "\n",
    "**Note**: opencv reads in image using BGR ordering, so we need to reverse it. Hint:  use ``::-1`` to reverse the items\n",
    "4. Add additional dimension as 1st dimension as the model expects to receive inputs in batches, i.e of shape (batch, width, height, channels) ***Hint*** use ```np.expand_dims()```\n",
    "5. Call model.predict() to get the predictions of shape (13, 13, 5, 85)\n",
    "6. Pass the predictions to decode_netout()  **Hint** remove the 1st axis (batch axis) before call decode_netout()\n",
    "7. Call non_max_suppression() to get the final list of boxes\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "input_image = cv2.resize(image, (416, 416))\n",
    "input_image = input_image / 255.\n",
    "input_image = input_image[:,:,::-1]\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "netout = model.predict(input_image)\n",
    "\n",
    "boxes = decode_netout(netout[0], \n",
    "                      anchors=ANCHOR_BOXES, \n",
    "                      nb_class=NUM_CLASSES)\n",
    "\n",
    "boxes = non_max_suppression(boxes, NUM_CLASSES, NMS_THRESHOLD, OBJ_THRESHOLD)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:19:07.263359",
     "start_time": "2018-04-04T00:19:05.658285"
    }
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('data/giraffe.jpg')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "### END THE CODE  ###\n",
    "\n",
    "## draw the box on the original image, not preprocessed image\n",
    "image = draw_boxes(image, boxes, labels=labels)\n",
    "\n",
    "## reverse the BGR to RGB channel ordering\n",
    "plt.imshow(image[:,:,::-1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform detection on video\n",
    "\n",
    "The following code shows how to perform detection on video and write the result (image with drawn bounding boxes) to an image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-06T13:39:09.640646Z",
     "start_time": "2017-10-06T13:31:44.627609Z"
    }
   },
   "outputs": [],
   "source": [
    "video_inp = 'data/street.mp4'\n",
    "video_out = 'data/street_predicted.mp4'\n",
    "\n",
    "video_reader = cv2.VideoCapture(video_inp)\n",
    "\n",
    "nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "video_writer = cv2.VideoWriter(video_out,\n",
    "                               cv2.VideoWriter_fourcc(*'XVID'), \n",
    "                               30.0, \n",
    "                               (frame_w, frame_h))\n",
    "\n",
    "for i in tqdm(range(nb_frames)):\n",
    "    ret, image = video_reader.read()\n",
    "    \n",
    "    input_image = cv2.resize(image, (416, 416))\n",
    "    input_image = input_image / 255.\n",
    "    input_image = input_image[:,:,::-1]\n",
    "    input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "    netout = model.predict(input_image)\n",
    "\n",
    "    boxes = decode_netout(netout[0], \n",
    "                          obj_threshold=0.3,\n",
    "                          nms_threshold=NMS_THRESHOLD,\n",
    "                          anchors=ANCHOR_BOXES, \n",
    "                          nb_class=NUM_CLASSES)\n",
    "    image = draw_boxes(image, boxes, labels=labels)\n",
    "\n",
    "    video_writer.write(np.uint8(image))\n",
    "    \n",
    "video_reader.release()\n",
    "video_writer.release()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's playback the video that we have created. \n",
    "\n",
    "***Note***: \n",
    "\n",
    "Only run the cell below if if you are running on a local PC. The opencv needs to open a local window to play the video and this is not possible if you remotely access the server through a browser (e.g. when you are using cloud VM). So, if you are using the cloud VM, you can download the video to your local PC and play it using any video player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the video file and play it\n",
    "video_out = 'data/street_predicted.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_out)\n",
    "\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error')\n",
    "cv2.namedWindow('Frame')\n",
    "cv2.startWindowThread()    \n",
    "while(cap.isOpened()):   \n",
    "    ret, frame = cap.read() \n",
    "    cv2.startWindowThread()\n",
    "    if ret == True: \n",
    "        # Display the resulting frame \n",
    "        cv2.imshow('Frame', frame) \n",
    "\n",
    "        # Press Q on keyboard to  exit \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Exercise**\n",
    "\n",
    "Try using your own image or video to do Object Detection, have FUN!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (tf1env)",
   "language": "python",
   "name": "tf1env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {
    "height": "122px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "758px",
    "left": "0px",
    "right": "1096px",
    "top": "73px",
    "width": "253px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
