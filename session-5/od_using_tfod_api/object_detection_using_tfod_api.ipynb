{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-pdc2-students/blob/master/iti107/session-5/od_using_tfod_api/object_detection_using_tfod_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection using Tensorflow Object Detection API (aka TFOD API)\n",
    "\n",
    "Welcome to the programming exercise of 'Object Detection using TFOD API'.  This notebook will walk you through, step by step, the process of using the TFOD API for object detection.\n",
    "\n",
    "Before you can run the codes in this notebook, ensure the TFOD API has been installed. If you are using the lab machine or the cloud VM that is provided, the TFOD API has been already been installed. If you are using your own machine, make sure to follow the [TFOD API installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start. \n",
    "\n",
    "Ensure that you are using Tensorflow 1.14 environment.\n",
    "\n",
    "***Credit: This notebook is adapted from the Object Detection Tutorial in the TFOD API.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from shutil import copy2\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tensorflow object detection API \n",
    "\n",
    "As Tensorflow object detection library is not installed as PIP package, we need to specifically tell the Python interpreter where to look for the different modules in the object detection package.  We need to add the install directory to the search path of the Python interpreter by appending them to the `sys.path` or the environment variable `PYTHONPATH`. Modify the following according to your own system environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy72mWwAWKMK"
   },
   "source": [
    "## 2. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root Tensorflow model directory. Modify this accordingly\n",
    "#TF_MODELS_RESEARCH_DIR = '/home/ubuntu/git/tensorflow/models/research'\n",
    "TF_MODELS_RESEARCH_DIR = '/Users/markk/git/tensorflow/models/research'\n",
    "TF_SLIM_DIR = os.path.join(TF_MODELS_RESEARCH_DIR, 'slim')\n",
    "TF_OD_DIR = os.path.join(TF_MODELS_RESEARCH_DIR, 'object_detection')\n",
    "\n",
    "sys.path.append(TF_MODELS_RESEARCH_DIR)\n",
    "sys.path.append(TF_SLIM_DIR)\n",
    "sys.path.append(TF_OD_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "### TFOD API imports\n",
    "Here are the imports of the required object detection modules in TFOD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v7m_NY_aWKMK"
   },
   "outputs": [],
   "source": [
    "from utils import ops as utils_ops\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "## 3. Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "### Choose the detection model\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool of TFOD_API can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.   We will cover the export_inference_graph tool in the next exercise when we do our own custom training.\n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies. Note the filename of the downloaded file is in the format of \\<model name\\>.tar.gz, e.g. **faster_rcnn_resnet50_coco_2018_01_28.tar.gz**. Change the variable **MODEL_NAME** below to the \\<model name\\>, e.g. **faster_rcnn_resnet50_coco_2018_01_28**. \n",
    "\n",
    "You will also need to provide the path to the appropriate label map file (explained later in 'Loading Label Map'). A list of label map files (with the file suffix .pbtxt) is provided in the `data` subfolder in the TFOD API object detection folder. So depending on the model you chose, copy the mapping file (.pbtxt) to appropriate working directory (e.g. the current directory of this notebook). In the example below, since we chose the model 'ssd_mobilenet_v1_coco_2017_11_17' which is trained on mscoco dataset, we will copy the file 'mscoco_label_map.pbtxt' to the current directory. If you train your own custom detection model, you will need to provide your own label map file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "LABEL_FILE = 'mscoco_label_map.pbtxt'\n",
    "PATH_TO_LABELS = os.path.join(TF_OD_DIR, 'data',LABEL_FILE)\n",
    "\n",
    "copy2(PATH_TO_LABELS, LABEL_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ai8pLZZWKMS"
   },
   "source": [
    "### Download Model\n",
    "\n",
    "Now we download the pre-trained model from the model zoo and extract the frozen graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KILYnwR5WKMS"
   },
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBcB9QHLWKMU"
   },
   "source": [
    "### Load the (frozen) Tensorflow model into memory.\n",
    "\n",
    "We will now load the frozne graph (downloaded earlier from model zoo) into the memory. A frozen graph is a tensorflow graph that cannot be trained (hence the word frozen) and is meant for inference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KezjCRVvWKMV"
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "### Loading label map\n",
    "A 'Label map' maps indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility function of TFOD API, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "## 4. Object Detection on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFsoUHvbWKMZ"
   },
   "source": [
    "### Helper code\n",
    "\n",
    "The image is read using Pillow as an Image object. Image.size gives the dimension of image as widht, height ordering. `Image.getdata()` gives a flattened array of bytes, so we need to reshape it to `(height, width, channels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aSlYc3JkWKMa"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models that are trained with TFOD API, some standard tensor names are used, e.g. num_detections, detection_boxes, 'detection_scores', 'detection_classes', etc. \n",
    "\n",
    "The following codes assume the presence of the following tensors \n",
    "\n",
    "- detection_boxes: coordinates of the detection boxes in the image.\n",
    "- detection_scores: detection scores for the detection boxes in the image.\n",
    "- detection_classes: detection-level class labels.\n",
    "- num_detections: number of detections in the batch.\n",
    "\n",
    "In our case, our training specifies maximum total detections (max_total_detections) of 100 and also maximum detections per class (max_detections_per_class) of 100, the output tensors for detection_scores, detection_classes are of the shape (?,100) and for the detection_boxes it is (?, 100, 4) where the 4 refer to the diagonal corners of the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read the image file using pillow Image class.  Remember that our network always expect the tensors to be fed in batches, we need to add additional dimension as first axis, by calling np.expand_dims(x, axis=0).\n",
    "\n",
    "We then call the `run_inference_for_single_image()` defined above to get the predicted bounding boxes and classes.  We use the utility function provided by TFOD API: visualization_utils.visualize_boxes_and_labels_on_image_array() to draw the boxes on the image. We can control the score threshold for a box to be visualized by changing the `min_score_thresh` parameter value. \n",
    "\n",
    "If the label text is not clear or illegible, you may want to change the font used by the `visualize_boxes_and_labels_on_image_array()`. By default, it will try to load the font called arial.ttf and if there is an error in loading, it will then call `ImageFont.load_default()` and this default font may not be legible on certain platform (e.g. MacOS).  For more info on ImageFont, refers to [PIL documentation](https://pillow.readthedocs.io/en/stable/reference/ImageFont.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "92BHxzcNWKMf"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image_path, graph):\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "            image_tensor = graph.get_tensor_by_name('image_tensor:0')\n",
    "            detection_boxes = graph.get_tensor_by_name('detection_boxes:0')\n",
    "            detection_scores = graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "            # Run inference\n",
    "            \n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "                            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                            feed_dict={image_tensor: image_np_expanded})\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                        image_np,\n",
    "                        np.squeeze(boxes),\n",
    "                        np.squeeze(classes).astype(np.int32),\n",
    "                        np.squeeze(scores),\n",
    "                        category_index,\n",
    "                        min_score_thresh=0.4,\n",
    "                        use_normalized_coordinates=True,\n",
    "                        line_thickness=10)\n",
    "            \n",
    "\n",
    "            # Size, in inches, of the output images.\n",
    "            IMAGE_SIZE = (12, 8)\n",
    "            plt.figure(figsize=IMAGE_SIZE)\n",
    "            plt.imshow(image_np)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'data/dog.jpg'\n",
    "run_inference_for_single_image(image, detection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Object Detection on Video (Optional) \n",
    "\n",
    "Only run this when you are using a local computer, as the cv2 video player window is shown as a separate window on local computer, not within the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def run_inference_for_video(video_filepath, graph):\n",
    "    video_player = cv2.VideoCapture(video_filepath)\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            image_tensor = graph.get_tensor_by_name('image_tensor:0')\n",
    "            detection_boxes = graph.get_tensor_by_name('detection_boxes:0')\n",
    "            detection_scores = graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "            while video_player.isOpened():\n",
    "                ret, image_np = video_player.read()\n",
    "                if ret:\n",
    "                    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "                    (boxes, scores, classes, num) = sess.run(\n",
    "                      [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                      feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "                    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                        image_np,\n",
    "                        np.squeeze(boxes),\n",
    "                        np.squeeze(classes).astype(np.int32),\n",
    "                        np.squeeze(scores),\n",
    "                        category_index,\n",
    "                        use_normalized_coordinates=True,\n",
    "                        line_thickness=10)\n",
    "\n",
    "                    cv2.imshow('Object Detection', image_np)\n",
    "                    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "    # Release camera and close windows\n",
    "    video_player.release()\n",
    "    cv2.destroyAllWindows() \n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'data/dashcam.mp4'\n",
    "run_inference_for_video(video_file, detection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
