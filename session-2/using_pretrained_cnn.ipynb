{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-pdc2-students/blob/master/iti107/session-2/using_pretrained_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained CNN models\n",
    "\n",
    "Welcome to this week's programming exercise. We have covered many different Convolutional Neural Network architectures such as VGG, ResNet, Inception and MobileNet. It is time to see them in action. \n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- load pretrained models of some popular Convolutional Neural Networks and use them to classify images\n",
    "- identify some of the architecture patterns in the popular Convolutional Neural Network\n",
    "- compare the inference speed of different models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data\n",
    "\n",
    "If you want to test it with your own picture, download them to your local PC (or the server, if you are using the class VM) and change the img_path to point to your download picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following to your own image\n",
    "img_path = 'data/chair_table.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# Read Images \n",
    "img = mpimg.imread(img_path) \n",
    "  \n",
    "# Output Images \n",
    "plt.imshow(img) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 - Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "vgg16_model = vgg16.VGG16(weights='imagenet')\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Questions***\n",
    "\n",
    "1. What is the expected input image size?\n",
    "2. What are the last four layers in VGG-16? \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. it is expected to have a height of 224 and width of 224\n",
    "2. the last 4 layers are flatten (which flattens the 2-D array into 1-D array before feeding to FC layer), and 2 Fully-connected (Dense) layers, and the last layer is a soft-max layer to classify 1000-classes. This is quite typical of a image classifier.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function to Load Image, Preprocess input and Targets\n",
    "def predict_image(model, img_path, preprocess_input_fn, decode_predictions_fn, target_size=(224,224)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input_fn(x)\n",
    "    preds = model.predict(x)\n",
    "    predictions_df = pd.DataFrame(decode_predictions_fn(preds, top=10)[0])\n",
    "    predictions_df.columns = [\"Predicted Class\", \"Name\", \"Probability\"]\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path=\"rocking_chair.png\"  ## Uncomment this and put the path to your file here if desired\n",
    "# Predict Results\n",
    "predict_image(vgg16_model, img_path, vgg16.preprocess_input, vgg16.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we pass in `vgg.preprocess_input` function to preprocess the image before calling `model.predict()`. Different network (e.g. VGG, ResNet, etc) expects the input image to be normalized in different ways, and different models will provide their own preprocess_input() function to perform the normalization.\n",
    "\n",
    "We also call `np.expand_dims(x, axis=0)` before calling `preprocess_input()` and `predict()`. \n",
    "\n",
    "***Question***\n",
    "\n",
    "1. What does `np.expand_dims(x, axis=0)` do and why do we need it? \n",
    "2. Our sample picture consists of both table and chair? What does VGG16 predict? and why do you think it predicts so?\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "1. np.expand_dims() increases the number of dimensions and the axis of the new dimension is specified by the axis parameter. In this case, we add in a new axis as axis=0, first axis. This is because the preprocess_input() and predict() function expects the images to be in the shape (samples, height, width, channels), the 1st axis being the batch.\n",
    "\n",
    "2. It predicts dining table. It probably focus on the object in the middle of the image.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50 - Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will download the weights that might take a while\n",
    "# Also, the summary will be quite long, since Resnet50 is a much larger network than VGG16\n",
    "\n",
    "from tensorflow.keras.applications import resnet50\n",
    "\n",
    "resnet50_model = resnet50.ResNet50(weights='imagenet')\n",
    "\n",
    "resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Questions***\n",
    "\n",
    "1. Can you identify the skip connection block from the model summary()?\n",
    "2. Look at the last few layers in the ResNet. How are they different from those of VGG-16?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. Look for those 'Add' layer (e.g. layer with name add_2). The Add layer adds the skip connection with the previous layer. Notice that the add is done before the Activation function. You can also call plot_model() to get a graphical visualization of the model.\n",
    "\n",
    "2. ResNet does not use make use of Full-connected layers as classification layers. Instead it replaces the FC layers with GlobalAveragePooling2D. This architecture is very common in more modern architectures.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following to visualize the model in graphical form\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "#plot_model(resnet50_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Results\n",
    "predict_image(resnet50_model, img_path, resnet50.preprocess_input, resnet50.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet v1 - Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')\n",
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Questions***\n",
    "\n",
    "1. Can you identify the Depth-wise separable Convolution layer from the model summary()?\n",
    "2. How about the Point-wise convolution? \n",
    "3. Look at the last few layers in the MobileNet. How are they different from those of VGG-16?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "1. For example, the layer called 'conv_dw1'. \n",
    "\n",
    "2. For example, the layer called 'conv_pw1'. \n",
    "\n",
    "3. MobileNet does not use make use of Full-connected layers as classification layers. Instead it replaces the FC layers with GlobalAveragePooling2D. This architecture is very common in more modern architectures.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(mobilenet_model, img_path, mobilenet.preprocess_input, mobilenet.decode_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed comparison \n",
    "\n",
    "We compare the inference speed of the three different models. Which one has the fastest inference speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predict_image(vgg16_model, img_path, vgg16.preprocess_input, vgg16.decode_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predict_image(resnet50_model, img_path, resnet50.preprocess_input, resnet50.decode_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predict_image(mobilenet_model, img_path, mobilenet.preprocess_input, mobilenet.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Exercises \n",
    "\n",
    "1. Experiment with other networks such as InceptionV3 and compare the accuracy and speed with VGG/ResNet/MobileNet.\n",
    "2. Identify the architectual patterns used in such networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
